# ðŸš€ GPU Development Container Setup for RAGLite

This guide explains the GPU-enabled development container configuration for RAGLite, designed to leverage the NVIDIA GeForce RTX 3090 for accelerated embeddings and model inference.

## ðŸ—ï¸ Container Configuration

### Docker Compose Changes
The `docker-compose.yml` has been enhanced with:
- NVIDIA runtime support
- GPU device allocation
- Required environment variables
- Proper GPU resource reservations

### Dockerfile Enhancements
The container now includes:
- CUDA Toolkit 12.4 (compatible with older drivers)
- NVIDIA utilities and libraries
- Proper CUDA environment variables
- Build tools for compiling GPU-enabled packages

### Development Container Features
- Automatic GPU-enabled PyTorch installation (CUDA 12.4)
- llama-cpp-python with CUDA support
- Environment variables for GPU development
- Verification scripts for testing setup

## ðŸ”§ Key Components

### 1. NVIDIA Runtime
```yaml
runtime: nvidia
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]
```

### 2. CUDA Environment
```dockerfile
ENV CUDA_HOME=/usr/local/cuda-12.4
ENV PATH=$CUDA_HOME/bin:$PATH
ENV LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
```

### 3. GPU-Enabled Python Packages
```bash
# PyTorch with CUDA 12.4 support
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# llama-cpp-python with CUDA support
CMAKE_ARGS='-DLLAMA_CUDA=on' pip install llama-cpp-python --force-reinstall --no-cache-dir
```

## ðŸŽ¯ Hardware Target

**GPU**: NVIDIA GeForce RTX 3090
- **Memory**: 24GB GDDR6X
- **Architecture**: Ampere (GA102)
- **CUDA Compute Capability**: 8.6
- **Expected Performance**: 3-4x speedup for embedding generation

## ðŸš¦ Verification

### Quick Check
Run the verification script:
```bash
python scripts/verify_gpu_setup.py
```

### Manual Verification
```bash
# Check NVIDIA driver
nvidia-smi

# Check CUDA toolkit
nvcc --version

# Check PyTorch CUDA
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}'); print(f'Devices: {torch.cuda.device_count()}')"

# Check GPU memory
python -c "import torch; print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB')" 
```

## ðŸ“Š Performance Expectations

### Before (CPU Only)
- **Embedding Speed**: ~22 documents/second
- **Memory Usage**: System RAM only
- **Model Loading**: Slower, CPU-bound

### After (GPU Accelerated)
- **Embedding Speed**: ~80 documents/second (3.6x speedup)
- **Memory Usage**: ~4GB VRAM + system RAM
- **Model Loading**: Faster with GPU layers

## ðŸ”„ Rebuilding the Container

After making changes, rebuild the development container:

```bash
# Rebuild from scratch
docker-compose down
docker-compose build --no-cache
docker-compose up -d
```

Or in VS Code:
1. Open Command Palette (`Ctrl+Shift+P`)
2. Run "Dev Containers: Rebuild Container"

## ðŸ› Troubleshooting

### Common Issues

1. **"nvidia-smi" not working**
   - Ensure Docker has access to GPU
   - Check `--gpus all` flag or runtime configuration

2. **PyTorch CUDA not available**
   - Verify CUDA 12.4 compatibility
   - Check driver version (>= 550)

3. **llama-cpp-python without CUDA**
   - Ensure CMAKE_ARGS include `-DLLAMA_CUDA=on`
   - Rebuild package with `--force-reinstall`

4. **Version mismatches**
   - The container uses CUDA 12.4 for better compatibility
   - Host driver 570.x should support container CUDA 12.4

### Environment Variables
```bash
export CUDA_HOME=/usr/local/cuda-12.4
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
export NVIDIA_VISIBLE_DEVICES=all
```

## ðŸŽª Usage in RAGLite

The GPU acceleration will be automatically detected and used:

```python
from raglite._config import RAGLiteConfig
from raglite._gpu_utils import detect_cuda_availability

# Configure with GPU support
config = RAGLiteConfig(gpu_enabled=True)

# Check GPU availability
if detect_cuda_availability():
    print("ðŸš€ GPU acceleration enabled!")
else:
    print("ðŸ’» Falling back to CPU")
```

## ðŸ“ˆ Expected Performance Improvements

| Component | CPU Performance | GPU Performance | Speedup |
|-----------|----------------|-----------------|---------|
| Embeddings | 22 docs/sec | 80 docs/sec | 3.6x |
| Model Loading | ~30s | ~10s | 3x |
| Inference | Variable | 2-4x faster | 2-4x |

This setup provides a robust, GPU-accelerated development environment that automatically handles compatibility issues while maximizing performance on the RTX 3090.
